{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPvYEl/ys8LOinciNQpJChu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b-harr/dmcb/blob/colab/DMCB_Python_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## scrape_bbref.py\n",
        "\n",
        "Run this script as often as necessary to get live* statistics from BBRef\n",
        "\n",
        "*Update frequency determined by 3rd party"
      ],
      "metadata": {
        "id": "paQHx1PqziQV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKGVaRwpy1Pv"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Function to clean a player's name and generate a unique player key to join across sites\n",
        "# Normalizes the name (e.g., replaces accents with ASCII), converts to lowercase, strips trailing spaces,\n",
        "# replaces spaces with hyphens, removes special characters (e.g., periods and apostrophes), and strips\n",
        "# suffixes (e.g., \"-jr\", \"-iii\").\n",
        "def make_player_key(name):\n",
        "    normalized_text = unicodedata.normalize(\"NFD\", name).encode(\"ascii\", \"ignore\").decode(\"utf-8\")  # Remove accents\n",
        "    cleaned_name = normalized_text.lower().strip()  # Convert to lowercase and remove trailing spaces\n",
        "    cleaned_name = re.sub(r\"\\s+\", \"-\", cleaned_name)  # Replace spaces with hyphens\n",
        "    cleaned_name = re.sub(r\"[^\\w-]\", \"\", cleaned_name)  # Remove non-alphanumeric characters\n",
        "    player_key = re.sub(r\"-(sr|jr|ii|iii|iv|v|vi|vii)$\", \"\", cleaned_name)  # Remove common suffixes\n",
        "    return player_key\n",
        "\n",
        "# Define the URL\n",
        "url = \"https://www.basketball-reference.com/leagues/NBA_2025_totals.html\"\n",
        "\n",
        "# Send a GET request to the URL\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code != 200:\n",
        "    print(f\"Failed to fetch data: {response.status_code}\")\n",
        "    exit()\n",
        "\n",
        "# Parse the HTML\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "# Find the stats table\n",
        "table = soup.find(\"table\", {\"id\": \"totals_stats\"})\n",
        "if not table:\n",
        "    print(\"Table not found. Ensure the page structure has not changed.\")\n",
        "    exit()\n",
        "\n",
        "# Extract the table headers\n",
        "headers = [th.get_text() for th in table.find(\"thead\").find_all(\"th\")]\n",
        "headers = headers[1:]  # Remove the first blank column header\n",
        "\n",
        "# Extract the rows\n",
        "rows = table.find(\"tbody\").find_all(\"tr\")\n",
        "data = []\n",
        "for row in rows:\n",
        "    # Skip rows without data (e.g., separator rows)\n",
        "    if row.find(\"td\"):\n",
        "        row_data = [td.get_text() for td in row.find_all(\"td\")]\n",
        "        data.append(row_data)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data, columns=headers)\n",
        "\n",
        "# Filter out 'League Average' from the 'Player' column\n",
        "df = df[df[\"Player\"] != \"League Average\"]\n",
        "\n",
        "# Add 'Player Key' column by applying the make_player_key function to the 'Player' column\n",
        "df[\"Player Key\"] = df[\"Player\"].apply(make_player_key)\n",
        "\n",
        "# Sort by 'Player Key' and 'Team' columns\n",
        "df = df.sort_values(by=[\"Player Key\", \"Team\"])\n",
        "\n",
        "# Save to CSV\n",
        "output_csv = \"bbref_data.csv\"\n",
        "df.to_csv(output_csv, index=False, quoting=1)\n",
        "\n",
        "# Get the current datetime in the local timezone\n",
        "import pytz\n",
        "import datetime\n",
        "timezone = pytz.timezone(\"America/Chicago\")  # Replace with your local timezone\n",
        "current_time = datetime.datetime.now(timezone).strftime(\"%Y-%m-%d %H:%M:%S %Z%z\")\n",
        "\n",
        "# Print the completion message with timestamp and timezone\n",
        "print(f\"Data saved to {output_csv} at {current_time}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## scrape_salary.py\n",
        "\n",
        "Run this script as often as needed to get multi-year salary data for all 30 NBA teams from Spotrac."
      ],
      "metadata": {
        "id": "K6dWn_tQzpl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# List of NBA teams to scrape salary data for\n",
        "# Each entry corresponds to the team's Spotrac URL identifier, e.g., \"https://www.spotrac.com/nba/atlanta-hawks/yearly\"\n",
        "teams = [\n",
        "    \"atlanta-hawks\", \"brooklyn-nets\", \"boston-celtics\", \"charlotte-hornets\",\n",
        "    \"cleveland-cavaliers\", \"chicago-bulls\", \"dallas-mavericks\", \"denver-nuggets\",\n",
        "    \"detroit-pistons\", \"golden-state-warriors\", \"houston-rockets\", \"indiana-pacers\",\n",
        "    \"la-clippers\", \"los-angeles-lakers\", \"memphis-grizzlies\", \"miami-heat\",\n",
        "    \"milwaukee-bucks\", \"minnesota-timberwolves\", \"new-york-knicks\",\n",
        "    \"new-orleans-pelicans\", \"oklahoma-city-thunder\", \"orlando-magic\",\n",
        "    \"philadelphia-76ers\", \"phoenix-suns\", \"portland-trail-blazers\",\n",
        "    \"san-antonio-spurs\", \"sacramento-kings\", \"toronto-raptors\",\n",
        "    \"utah-jazz\", \"washington-wizards\"\n",
        "]\n",
        "\n",
        "# Function to clean a player's name and generate a unique player key to join across sites\n",
        "# Normalizes the name (e.g., replaces accents with ASCII), converts to lowercase, strips trailing spaces,\n",
        "# replaces spaces with hyphens, removes special characters (e.g., periods and apostrophes), and strips\n",
        "# suffixes (e.g., \"-jr\", \"-iii\").\n",
        "def make_player_key(name):\n",
        "    normalized_text = unicodedata.normalize(\"NFD\", name).encode(\"ascii\", \"ignore\").decode(\"utf-8\")  # Remove accents\n",
        "    cleaned_name = normalized_text.lower().strip()  # Convert to lowercase and remove trailing spaces\n",
        "    cleaned_name = re.sub(r\"\\s+\", \"-\", cleaned_name)  # Replace spaces with hyphens\n",
        "    cleaned_name = re.sub(r\"[^\\w-]\", \"\", cleaned_name)  # Remove non-alphanumeric characters\n",
        "    player_key = re.sub(r\"-(sr|jr|ii|iii|iv|v|vi|vii)$\", \"\", cleaned_name)  # Remove common suffixes\n",
        "    return player_key\n",
        "\n",
        "# Function to extract and clean the team name from the Spotrac URL\n",
        "# Formats the team name from the URL (e.g., \"san-antonio-spurs\" -> \"San Antonio Spurs\")\n",
        "def clean_team_name(url):\n",
        "    team_key = url.split(\"/\")[-2]  # Extracts the team identifier from the URL\n",
        "    team_key_parts = team_key.split(\"-\")  # Splits the identifier into components\n",
        "    # Capitalizes each word, with special handling\n",
        "    formatted_name = \" \".join(\n",
        "        part.upper() if part.lower() == \"la\"  # Capitalize \"LA\" specifically (e.g. \"Los Angeles\")\n",
        "        else part.capitalize() if part.isalpha()  # Capitalize alphabetic parts only (e.g., \"Spurs\")\n",
        "        else part  # Retain numeric parts as they are (e.g., \"76ers\")\n",
        "        for part in team_key_parts\n",
        "    )\n",
        "    return formatted_name\n",
        "\n",
        "# File path for saving the output CSV\n",
        "output_csv = \"salary_data.csv\"\n",
        "\n",
        "# List to store all salary data collected during scraping\n",
        "all_data = []\n",
        "\n",
        "# Function to extract dynamic season headers from a team's salary table\n",
        "# This ensures the script captures season columns dynamically\n",
        "def extract_season_headers(teams):\n",
        "    for team in teams:\n",
        "        url = f\"https://www.spotrac.com/nba/{team}/yearly\"\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:  # Check if the request is successful\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            table = soup.select_one(\"table\")  # Locate the first table in the page\n",
        "            if table:\n",
        "                header_row = table.find(\"tr\")  # Find the header row\n",
        "                if header_row:\n",
        "                    headers = [th.get_text(strip=True) for th in header_row.find_all(\"th\")]\n",
        "                    # Filter headers matching the season format \"YYYY-YY\"\n",
        "                    season_headers = [header for header in headers if re.match(r\"^\\d{4}-\\d{2}$\", header)]\n",
        "                    if season_headers:  # Return headers if found\n",
        "                        print(f\"Season headers extracted from team: {clean_team_name(url)}\")\n",
        "                        return season_headers\n",
        "    print(\"Failed to extract season headers. Please check the team URLs or table structure.\")\n",
        "    return []  # Return an empty list if no headers are found\n",
        "\n",
        "# Extract headers dynamically from the list of teams\n",
        "season_headers = extract_season_headers(teams)\n",
        "if not season_headers:\n",
        "    raise ValueError(\"Season headers could not be determined. Check table structure or team data.\")\n",
        "\n",
        "# Define CSV headers for the output file\n",
        "headers = [\"Player\", \"Player Link\", \"Player Key\", \"Team\", \"Team Link\", \"Position\", \"Age\"] + season_headers\n",
        "# Create an empty CSV file with the defined headers\n",
        "pd.DataFrame(columns=headers).to_csv(output_csv, index=False, mode=\"w\", encoding=\"utf-8\", quoting=1)\n",
        "\n",
        "# Loop through each team to scrape data\n",
        "total_teams = len(teams)\n",
        "for idx, team in enumerate(teams):\n",
        "    url = f\"https://www.spotrac.com/nba/{team}/yearly\"  # Construct the team's URL\n",
        "    team_name = clean_team_name(url)  # Extract and clean the team name\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:  # If the request is successful\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        table = soup.select_one(\"table\")  # Locate the salary table\n",
        "\n",
        "        if table:\n",
        "            rows = table.find_all(\"tr\")  # Extract all rows from the table\n",
        "            for row in rows[1:]:  # Skip the header row\n",
        "                cols = row.find_all(\"td\")  # Extract all columns for the row\n",
        "                player_name = \"\"\n",
        "                player_link = \"\"\n",
        "                position = \"\"\n",
        "                age = \"\"\n",
        "                salary_data = []\n",
        "\n",
        "                if len(cols) > 0:\n",
        "                    player_name_tag = cols[0].find(\"a\")  # Find the player link in the first column\n",
        "                    if player_name_tag:\n",
        "                        player_name = player_name_tag.get_text(strip=True)\n",
        "                        player_link = player_name_tag[\"href\"]\n",
        "                    player_key = make_player_key(player_name)  # Generate the player key\n",
        "                else:\n",
        "                    player_key = \"\"\n",
        "\n",
        "                if len(cols) > 1:  # Extract the player's position\n",
        "                    position = cols[1].get_text(strip=True)\n",
        "                if len(cols) > 2:  # Extract the player's age\n",
        "                    age = cols[2].get_text(strip=True)\n",
        "\n",
        "                for col in cols[3:]:  # Extract salary data from remaining columns\n",
        "                    cell_text = col.get_text(strip=True)\n",
        "                    if \"Two-Way\" in cell_text:\n",
        "                        salary_data.append(\"Two-Way\")\n",
        "                    elif \"UFA\" in cell_text:\n",
        "                        salary_data.append(\"UFA\")\n",
        "                    elif \"RFA\" in cell_text:\n",
        "                        salary_data.append(\"RFA\")\n",
        "                    else:  # Extract numeric salary values\n",
        "                        salary_matches = re.findall(r\"\\$\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?\", cell_text)\n",
        "                        salary_data.extend(salary_matches)\n",
        "\n",
        "                # Combine all collected data into a single row\n",
        "                salary_data = [player_name, player_link, player_key, team_name, url, position, age] + salary_data\n",
        "                salary_data += [\"\"] * (len(headers) - len(salary_data))  # Ensure row matches the header length\n",
        "\n",
        "                if salary_data[0]:  # Only save data if player name exists\n",
        "                    all_data.append(salary_data)\n",
        "                    pd.DataFrame([salary_data], columns=headers).to_csv(output_csv, index=False, mode=\"a\", header=False, encoding=\"utf-8\", quoting=1)\n",
        "\n",
        "        print(f\"Processed {idx + 1}/{total_teams} teams ({((idx + 1) / total_teams) * 100:.2f}%): {team_name}\")\n",
        "\n",
        "# Sort all data by the player key for consistency\n",
        "sorted_data = sorted(all_data, key=lambda x: x[2].lower())\n",
        "# Overwrite the CSV with sorted data\n",
        "pd.DataFrame(sorted_data, columns=headers).to_csv(output_csv, index=False, mode=\"w\", encoding=\"utf-8\", quoting=1)\n",
        "\n",
        "# Get the current datetime in the local timezone\n",
        "import pytz\n",
        "import datetime\n",
        "timezone = pytz.timezone(\"America/Chicago\")  # Replace with your local timezone\n",
        "current_time = datetime.datetime.now(timezone).strftime(\"%Y-%m-%d %H:%M:%S %Z%z\")\n",
        "\n",
        "# Print the completion message with timestamp and timezone\n",
        "print(f\"Data saved to {output_csv} at {current_time}\")\n"
      ],
      "metadata": {
        "id": "aJ7sUuJ5zZmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## scrape_signed.py\n",
        "\n",
        "Run this script infrequently on a local machine to loop through all active players and scrape their individual page for details regarding how they were signed."
      ],
      "metadata": {
        "id": "vj2Si4ePzxin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Input CSV file containing the salary data\n",
        "input_csv = \"salary_data.csv\"\n",
        "# Read the salary data from the input file into a pandas DataFrame\n",
        "salary_data = pd.read_csv(input_csv)\n",
        "\n",
        "# Filter out inactive players or those with \"Two-Way\" contracts\n",
        "active_data = salary_data[(salary_data[\"2024-25\"] != \"Two-Way\") & (salary_data[\"2024-25\"] != \"-\")]\n",
        "\n",
        "# Extract unique player links and keys, and sort by player key for consistency\n",
        "unique_links = active_data.drop_duplicates(subset=[\"Player Link\", \"Player Key\"]).sort_values(by=\"Player Key\")[\"Player Link\"].tolist()\n",
        "\n",
        "# List of minor words that should not be capitalized unless they are at the beginning of a phrase\n",
        "minor_words = {\"and\", \"or\", \"the\", \"in\", \"at\", \"for\", \"to\", \"by\", \"with\", \"a\", \"an\", \"of\", \"on\"}\n",
        "\n",
        "# Capitalizes specific prefixes and applies title case to the rest of the text\n",
        "def format_signed(text):\n",
        "    # If the text is None, return None\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    # Split the text into words by spaces or hyphens\n",
        "    words = re.split(r\"[-\\s]\", text)\n",
        "    formatted_words = []\n",
        "\n",
        "    # Capitalize words based on specific conditions\n",
        "    for i, word in enumerate(words):\n",
        "        # If the word starts with \"non\", \"mid\", or \"bi\", capitalize it (e.g., \"Non-\" becomes \"Non\")\n",
        "        if any(word.lower().startswith(prefix) for prefix in (\"non\", \"mid\", \"bi\")):\n",
        "            formatted_words.append(word.capitalize())\n",
        "        # Capitalize all other words unless they are minor words\n",
        "        else:\n",
        "            formatted_words.append(word if word.lower() in minor_words else word.capitalize())\n",
        "\n",
        "    # Join the formatted words into a single string\n",
        "    formatted = \" \".join(formatted_words)\n",
        "\n",
        "    # Replace the capitalization for \"Non-\", \"Mid-\", \"Bi-\" if needed\n",
        "    formatted = re.sub(r\"(?<=\\w)(?=\\b(?:Non|Mid|Bi)-)\", \"-\", formatted)\n",
        "\n",
        "    # Remove space after \"Non \", \"Mid \", \"Bi \" and replace it with a hyphen\n",
        "    formatted = re.sub(r\"(Non|Mid|Bi)\\s\", r\"\\1-\", formatted)\n",
        "\n",
        "    # Special case: Handle \"Sign and Trade\" as a unique exception\n",
        "    formatted = re.sub(r\"Sign and Trade\", \"Sign-and-Trade\", formatted)\n",
        "\n",
        "    return formatted\n",
        "\n",
        "# Function to scrape player data from the player's individual page\n",
        "def scrape_player_data(player_link, player_key, player_name):\n",
        "    try:\n",
        "        # Send a GET request to the player's page\n",
        "        page = requests.get(player_link)\n",
        "        soup = BeautifulSoup(page.content, \"html.parser\")  # Parse the HTML content of the page\n",
        "\n",
        "        # CSS selector to find the \"Signed Using\" contract information\n",
        "        signed_using_selector = \"#contracts > div > div > div.contract-wrapper.mb-5 > div.contract-details.row.m-0 > div:nth-child(5) > div.label\"\n",
        "        # Find the corresponding HTML element using the selector\n",
        "        signed_using_element = soup.select_one(signed_using_selector)\n",
        "\n",
        "        # Get the text of the next sibling element containing the actual contract information\n",
        "        signed_using_value = signed_using_element.find_next_sibling().get_text().strip() if signed_using_element else None\n",
        "\n",
        "        # Format the extracted contract data using the format_signed function\n",
        "        cleaned_value = format_signed(signed_using_value)\n",
        "\n",
        "        # Return a dictionary containing the player data with the cleaned \"Signed Using\" value\n",
        "        return {\n",
        "            \"Player\": player_name,\n",
        "            \"Player Link\": player_link,\n",
        "            \"Player Key\": player_key,\n",
        "            \"Signed Using\": cleaned_value\n",
        "        }\n",
        "    except Exception as e:\n",
        "        # If an error occurs (e.g., page structure changes), return None for contract data\n",
        "        return {\n",
        "            \"Player\": player_name,\n",
        "            \"Player Link\": player_link,\n",
        "            \"Player Key\": player_key,\n",
        "            \"Signed Using\": None\n",
        "        }\n",
        "\n",
        "# Output file where the scraped data will be saved\n",
        "output_csv = \"signed_data.csv\"\n",
        "# Initialize the output CSV file with headers\n",
        "pd.DataFrame(columns=[\"Player\", \"Player Link\", \"Player Key\", \"Signed Using\"]).to_csv(output_csv, index=False, mode=\"w\", encoding=\"utf-8\", quoting=1)\n",
        "\n",
        "# Loop through each unique player link and scrape the data\n",
        "for idx, link in enumerate(unique_links):\n",
        "    # Extract player key and player name from the active data DataFrame\n",
        "    player_key = active_data[active_data[\"Player Link\"] == link][\"Player Key\"].values[0]\n",
        "    player_name = active_data[active_data[\"Player Link\"] == link][\"Player\"].values[0]\n",
        "\n",
        "    # Scrape the player's contract data using the scrape_player_data function\n",
        "    scraped_row = scrape_player_data(link, player_key, player_name)\n",
        "    # Append the scraped data to the output CSV file, replacing the \"Signed Using\" column with the cleaned data\n",
        "    pd.DataFrame([scraped_row]).to_csv(output_csv, mode=\"a\", header=False, index=False, encoding=\"utf-8\", quoting=1)\n",
        "\n",
        "    # Print progress as players are processed\n",
        "    print(f\"Processed {idx + 1}/{len(unique_links)} players ({((idx + 1) / len(unique_links)) * 100:.2f}%): {player_name}\")\n",
        "\n",
        "# Get the current datetime in the local timezone\n",
        "import pytz\n",
        "import datetime\n",
        "timezone = pytz.timezone(\"America/Chicago\")  # Replace with your local timezone\n",
        "current_time = datetime.datetime.now(timezone).strftime(\"%Y-%m-%d %H:%M:%S %Z%z\")\n",
        "\n",
        "# Print the completion message with timestamp and timezone\n",
        "print(f\"Data saved to {output_csv} at {current_time}\")\n"
      ],
      "metadata": {
        "id": "QVHmu5ytzelw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}