{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPvYEl/ys8LOinciNQpJChu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b-harr/dmcb/blob/colab/DMCB_Python_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## scrape_bbref.py\n",
        "\n",
        "Run this script as often as necessary to get live* statistics from BBRef\n",
        "\n",
        "*Update frequency determined by 3rd party"
      ],
      "metadata": {
        "id": "paQHx1PqziQV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKGVaRwpy1Pv",
        "outputId": "20d5daf0-dd73-4b7a-e8a0-12ba2acc5b23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to bbref_data.csv at 2024-11-17 14:30:13 CST-0600\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import re\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Function to clean a player's name and generate a unique player key to join across sites\n",
        "# Normalizes the name (e.g., replaces accents with ASCII), converts to lowercase, strips trailing spaces,\n",
        "# replaces spaces with hyphens, removes special characters (e.g., periods and apostrophes), and strips\n",
        "# suffixes (e.g., \"-jr\", \"-iii\").\n",
        "def make_player_key(name):\n",
        "    normalized_text = unicodedata.normalize(\"NFD\", name).encode(\"ascii\", \"ignore\").decode(\"utf-8\")  # Remove accents\n",
        "    cleaned_name = normalized_text.lower().strip()  # Convert to lowercase and remove trailing spaces\n",
        "    cleaned_name = re.sub(r\"\\s+\", \"-\", cleaned_name)  # Replace spaces with hyphens\n",
        "    cleaned_name = re.sub(r\"[^\\w-]\", \"\", cleaned_name)  # Remove non-alphanumeric characters\n",
        "    player_key = re.sub(r\"-(sr|jr|ii|iii|iv|v|vi|vii)$\", \"\", cleaned_name)  # Remove common suffixes\n",
        "    return player_key\n",
        "\n",
        "# Define the URL\n",
        "url = \"https://www.basketball-reference.com/leagues/NBA_2025_totals.html\"\n",
        "\n",
        "# Send a GET request to the URL\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code != 200:\n",
        "    print(f\"Failed to fetch data: {response.status_code}\")\n",
        "    exit()\n",
        "\n",
        "# Parse the HTML\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "# Find the stats table\n",
        "table = soup.find(\"table\", {\"id\": \"totals_stats\"})\n",
        "if not table:\n",
        "    print(\"Table not found. Ensure the page structure has not changed.\")\n",
        "    exit()\n",
        "\n",
        "# Extract the table headers\n",
        "headers = [th.get_text() for th in table.find(\"thead\").find_all(\"th\")]\n",
        "headers = headers[1:]  # Remove the first blank column header\n",
        "\n",
        "# Extract the rows\n",
        "rows = table.find(\"tbody\").find_all(\"tr\")\n",
        "data = []\n",
        "for row in rows:\n",
        "    # Skip rows without data (e.g., separator rows)\n",
        "    if row.find(\"td\"):\n",
        "        row_data = [td.get_text() for td in row.find_all(\"td\")]\n",
        "        data.append(row_data)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data, columns=headers)\n",
        "\n",
        "# Filter out 'League Average' from the 'Player' column\n",
        "df = df[df[\"Player\"] != \"League Average\"]\n",
        "\n",
        "# Add 'Player Key' column by applying the make_player_key function to the 'Player' column\n",
        "df[\"Player Key\"] = df[\"Player\"].apply(make_player_key)\n",
        "\n",
        "# Sort by 'Player Key' and 'Team' columns\n",
        "df = df.sort_values(by=[\"Player Key\", \"Team\"])\n",
        "\n",
        "# Save to CSV\n",
        "output_csv = \"bbref_data.csv\"\n",
        "df.to_csv(output_csv, index=False, quoting=1)\n",
        "\n",
        "# Get the current datetime in the local timezone\n",
        "import pytz\n",
        "import datetime\n",
        "timezone = pytz.timezone(\"America/Chicago\")  # Replace with your local timezone\n",
        "current_time = datetime.datetime.now(timezone).strftime(\"%Y-%m-%d %H:%M:%S %Z%z\")\n",
        "\n",
        "# Print the completion message with timestamp and timezone\n",
        "print(f\"Data saved to {output_csv} at {current_time}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## scrape_salary.py\n",
        "\n",
        "Run this script as often as needed to get multi-year salary data for all 30 NBA teams from Spotrac."
      ],
      "metadata": {
        "id": "K6dWn_tQzpl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# List of NBA teams to scrape salary data for\n",
        "# Each entry corresponds to the team's Spotrac URL identifier, e.g., \"https://www.spotrac.com/nba/atlanta-hawks/yearly\"\n",
        "teams = [\n",
        "    \"atlanta-hawks\", \"brooklyn-nets\", \"boston-celtics\", \"charlotte-hornets\",\n",
        "    \"cleveland-cavaliers\", \"chicago-bulls\", \"dallas-mavericks\", \"denver-nuggets\",\n",
        "    \"detroit-pistons\", \"golden-state-warriors\", \"houston-rockets\", \"indiana-pacers\",\n",
        "    \"la-clippers\", \"los-angeles-lakers\", \"memphis-grizzlies\", \"miami-heat\",\n",
        "    \"milwaukee-bucks\", \"minnesota-timberwolves\", \"new-york-knicks\",\n",
        "    \"new-orleans-pelicans\", \"oklahoma-city-thunder\", \"orlando-magic\",\n",
        "    \"philadelphia-76ers\", \"phoenix-suns\", \"portland-trail-blazers\",\n",
        "    \"san-antonio-spurs\", \"sacramento-kings\", \"toronto-raptors\",\n",
        "    \"utah-jazz\", \"washington-wizards\"\n",
        "]\n",
        "\n",
        "# Function to clean a player's name and generate a unique player key to join across sites\n",
        "# Normalizes the name (e.g., replaces accents with ASCII), converts to lowercase, strips trailing spaces,\n",
        "# replaces spaces with hyphens, removes special characters (e.g., periods and apostrophes), and strips\n",
        "# suffixes (e.g., \"-jr\", \"-iii\").\n",
        "def make_player_key(name):\n",
        "    normalized_text = unicodedata.normalize(\"NFD\", name).encode(\"ascii\", \"ignore\").decode(\"utf-8\")  # Remove accents\n",
        "    cleaned_name = normalized_text.lower().strip()  # Convert to lowercase and remove trailing spaces\n",
        "    cleaned_name = re.sub(r\"\\s+\", \"-\", cleaned_name)  # Replace spaces with hyphens\n",
        "    cleaned_name = re.sub(r\"[^\\w-]\", \"\", cleaned_name)  # Remove non-alphanumeric characters\n",
        "    player_key = re.sub(r\"-(sr|jr|ii|iii|iv|v|vi|vii)$\", \"\", cleaned_name)  # Remove common suffixes\n",
        "    return player_key\n",
        "\n",
        "# Function to extract and clean the team name from the Spotrac URL\n",
        "# Formats the team name from the URL (e.g., \"san-antonio-spurs\" -> \"San Antonio Spurs\")\n",
        "def clean_team_name(url):\n",
        "    team_key = url.split(\"/\")[-2]  # Extracts the team identifier from the URL\n",
        "    team_key_parts = team_key.split(\"-\")  # Splits the identifier into components\n",
        "    # Capitalizes each word, with special handling\n",
        "    formatted_name = \" \".join(\n",
        "        part.upper() if part.lower() == \"la\"  # Capitalize \"LA\" specifically (e.g. \"Los Angeles\")\n",
        "        else part.capitalize() if part.isalpha()  # Capitalize alphabetic parts only (e.g., \"Spurs\")\n",
        "        else part  # Retain numeric parts as they are (e.g., \"76ers\")\n",
        "        for part in team_key_parts\n",
        "    )\n",
        "    return formatted_name\n",
        "\n",
        "# File path for saving the output CSV\n",
        "output_csv = \"salary_data.csv\"\n",
        "\n",
        "# List to store all salary data collected during scraping\n",
        "all_data = []\n",
        "\n",
        "# Function to extract dynamic season headers from a team's salary table\n",
        "# This ensures the script captures season columns dynamically\n",
        "def extract_season_headers(teams):\n",
        "    for team in teams:\n",
        "        url = f\"https://www.spotrac.com/nba/{team}/yearly\"\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:  # Check if the request is successful\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            table = soup.select_one(\"table\")  # Locate the first table in the page\n",
        "            if table:\n",
        "                header_row = table.find(\"tr\")  # Find the header row\n",
        "                if header_row:\n",
        "                    headers = [th.get_text(strip=True) for th in header_row.find_all(\"th\")]\n",
        "                    # Filter headers matching the season format \"YYYY-YY\"\n",
        "                    season_headers = [header for header in headers if re.match(r\"^\\d{4}-\\d{2}$\", header)]\n",
        "                    if season_headers:  # Return headers if found\n",
        "                        print(f\"Season headers extracted from team: {clean_team_name(url)}\")\n",
        "                        return season_headers\n",
        "    print(\"Failed to extract season headers. Please check the team URLs or table structure.\")\n",
        "    return []  # Return an empty list if no headers are found\n",
        "\n",
        "# Extract headers dynamically from the list of teams\n",
        "season_headers = extract_season_headers(teams)\n",
        "if not season_headers:\n",
        "    raise ValueError(\"Season headers could not be determined. Check table structure or team data.\")\n",
        "\n",
        "# Define CSV headers for the output file\n",
        "headers = [\"Player\", \"Player Link\", \"Player Key\", \"Team\", \"Team Link\", \"Position\", \"Age\"] + season_headers\n",
        "# Create an empty CSV file with the defined headers\n",
        "pd.DataFrame(columns=headers).to_csv(output_csv, index=False, mode=\"w\", encoding=\"utf-8\", quoting=1)\n",
        "\n",
        "# Loop through each team to scrape data\n",
        "total_teams = len(teams)\n",
        "for idx, team in enumerate(teams):\n",
        "    url = f\"https://www.spotrac.com/nba/{team}/yearly\"  # Construct the team's URL\n",
        "    team_name = clean_team_name(url)  # Extract and clean the team name\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:  # If the request is successful\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        table = soup.select_one(\"table\")  # Locate the salary table\n",
        "\n",
        "        if table:\n",
        "            rows = table.find_all(\"tr\")  # Extract all rows from the table\n",
        "            for row in rows[1:]:  # Skip the header row\n",
        "                cols = row.find_all(\"td\")  # Extract all columns for the row\n",
        "                player_name = \"\"\n",
        "                player_link = \"\"\n",
        "                position = \"\"\n",
        "                age = \"\"\n",
        "                salary_data = []\n",
        "\n",
        "                if len(cols) > 0:\n",
        "                    player_name_tag = cols[0].find(\"a\")  # Find the player link in the first column\n",
        "                    if player_name_tag:\n",
        "                        player_name = player_name_tag.get_text(strip=True)\n",
        "                        player_link = player_name_tag[\"href\"]\n",
        "                    player_key = make_player_key(player_name)  # Generate the player key\n",
        "                else:\n",
        "                    player_key = \"\"\n",
        "\n",
        "                if len(cols) > 1:  # Extract the player's position\n",
        "                    position = cols[1].get_text(strip=True)\n",
        "                if len(cols) > 2:  # Extract the player's age\n",
        "                    age = cols[2].get_text(strip=True)\n",
        "\n",
        "                for col in cols[3:]:  # Extract salary data from remaining columns\n",
        "                    cell_text = col.get_text(strip=True)\n",
        "                    if \"Two-Way\" in cell_text:\n",
        "                        salary_data.append(\"Two-Way\")\n",
        "                    elif \"UFA\" in cell_text:\n",
        "                        salary_data.append(\"UFA\")\n",
        "                    elif \"RFA\" in cell_text:\n",
        "                        salary_data.append(\"RFA\")\n",
        "                    else:  # Extract numeric salary values\n",
        "                        salary_matches = re.findall(r\"\\$\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?\", cell_text)\n",
        "                        salary_data.extend(salary_matches)\n",
        "\n",
        "                # Combine all collected data into a single row\n",
        "                salary_data = [player_name, player_link, player_key, team_name, url, position, age] + salary_data\n",
        "                salary_data += [\"\"] * (len(headers) - len(salary_data))  # Ensure row matches the header length\n",
        "\n",
        "                if salary_data[0]:  # Only save data if player name exists\n",
        "                    all_data.append(salary_data)\n",
        "                    pd.DataFrame([salary_data], columns=headers).to_csv(output_csv, index=False, mode=\"a\", header=False, encoding=\"utf-8\", quoting=1)\n",
        "\n",
        "        print(f\"Processed {idx + 1}/{total_teams} teams ({((idx + 1) / total_teams) * 100:.2f}%): {team_name}\")\n",
        "\n",
        "# Sort all data by the player key for consistency\n",
        "sorted_data = sorted(all_data, key=lambda x: x[2].lower())\n",
        "# Overwrite the CSV with sorted data\n",
        "pd.DataFrame(sorted_data, columns=headers).to_csv(output_csv, index=False, mode=\"w\", encoding=\"utf-8\", quoting=1)\n",
        "\n",
        "# Get the current datetime in the local timezone\n",
        "import pytz\n",
        "import datetime\n",
        "timezone = pytz.timezone(\"America/Chicago\")  # Replace with your local timezone\n",
        "current_time = datetime.datetime.now(timezone).strftime(\"%Y-%m-%d %H:%M:%S %Z%z\")\n",
        "\n",
        "# Print the completion message with timestamp and timezone\n",
        "print(f\"Data saved to {output_csv} at {current_time}\")\n"
      ],
      "metadata": {
        "id": "aJ7sUuJ5zZmO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49ffc871-9415-4e74-8a66-c567b2e4fb90"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Season headers extracted from team: Atlanta Hawks\n",
            "Processed 1/30 teams (3.33%): Atlanta Hawks\n",
            "Processed 2/30 teams (6.67%): Brooklyn Nets\n",
            "Processed 3/30 teams (10.00%): Boston Celtics\n",
            "Processed 4/30 teams (13.33%): Charlotte Hornets\n",
            "Processed 5/30 teams (16.67%): Cleveland Cavaliers\n",
            "Processed 6/30 teams (20.00%): Chicago Bulls\n",
            "Processed 7/30 teams (23.33%): Dallas Mavericks\n",
            "Processed 8/30 teams (26.67%): Denver Nuggets\n",
            "Processed 9/30 teams (30.00%): Detroit Pistons\n",
            "Processed 10/30 teams (33.33%): Golden State Warriors\n",
            "Processed 11/30 teams (36.67%): Houston Rockets\n",
            "Processed 12/30 teams (40.00%): Indiana Pacers\n",
            "Processed 13/30 teams (43.33%): LA Clippers\n",
            "Processed 14/30 teams (46.67%): Los Angeles Lakers\n",
            "Processed 15/30 teams (50.00%): Memphis Grizzlies\n",
            "Processed 16/30 teams (53.33%): Miami Heat\n",
            "Processed 17/30 teams (56.67%): Milwaukee Bucks\n",
            "Processed 18/30 teams (60.00%): Minnesota Timberwolves\n",
            "Processed 19/30 teams (63.33%): New York Knicks\n",
            "Processed 20/30 teams (66.67%): New Orleans Pelicans\n",
            "Processed 21/30 teams (70.00%): Oklahoma City Thunder\n",
            "Processed 22/30 teams (73.33%): Orlando Magic\n",
            "Processed 23/30 teams (76.67%): Philadelphia 76ers\n",
            "Processed 24/30 teams (80.00%): Phoenix Suns\n",
            "Processed 25/30 teams (83.33%): Portland Trail Blazers\n",
            "Processed 26/30 teams (86.67%): San Antonio Spurs\n",
            "Processed 27/30 teams (90.00%): Sacramento Kings\n",
            "Processed 28/30 teams (93.33%): Toronto Raptors\n",
            "Processed 29/30 teams (96.67%): Utah Jazz\n",
            "Processed 30/30 teams (100.00%): Washington Wizards\n",
            "Data saved to salary_data.csv at 2024-11-17 14:31:29 CST-0600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## scrape_signed.py\n",
        "\n",
        "Run this script infrequently on a local machine to loop through all active players and scrape their individual page for details regarding how they were signed."
      ],
      "metadata": {
        "id": "vj2Si4ePzxin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Input CSV file containing the salary data\n",
        "input_csv = \"salary_data.csv\"\n",
        "# Read the salary data from the input file into a pandas DataFrame\n",
        "salary_data = pd.read_csv(input_csv)\n",
        "\n",
        "# Filter out inactive players or those with \"Two-Way\" contracts\n",
        "active_data = salary_data[(salary_data[\"2024-25\"] != \"Two-Way\") & (salary_data[\"2024-25\"] != \"-\")]\n",
        "\n",
        "# Extract unique player links and keys, and sort by player key for consistency\n",
        "unique_links = active_data.drop_duplicates(subset=[\"Player Link\", \"Player Key\"]).sort_values(by=\"Player Key\")[\"Player Link\"].tolist()\n",
        "\n",
        "# List of minor words that should not be capitalized unless they are at the beginning of a phrase\n",
        "minor_words = {\"and\", \"or\", \"the\", \"in\", \"at\", \"for\", \"to\", \"by\", \"with\", \"a\", \"an\", \"of\", \"on\"}\n",
        "\n",
        "# Capitalizes specific prefixes and applies title case to the rest of the text\n",
        "def format_signed(text):\n",
        "    # If the text is None, return None\n",
        "    if text is None:\n",
        "        return None\n",
        "\n",
        "    # Split the text into words by spaces or hyphens\n",
        "    words = re.split(r\"[-\\s]\", text)\n",
        "    formatted_words = []\n",
        "\n",
        "    # Capitalize words based on specific conditions\n",
        "    for i, word in enumerate(words):\n",
        "        # If the word starts with \"non\", \"mid\", or \"bi\", capitalize it (e.g., \"Non-\" becomes \"Non\")\n",
        "        if any(word.lower().startswith(prefix) for prefix in (\"non\", \"mid\", \"bi\")):\n",
        "            formatted_words.append(word.capitalize())\n",
        "        # Capitalize all other words unless they are minor words\n",
        "        else:\n",
        "            formatted_words.append(word if word.lower() in minor_words else word.capitalize())\n",
        "\n",
        "    # Join the formatted words into a single string\n",
        "    formatted = \" \".join(formatted_words)\n",
        "\n",
        "    # Replace the capitalization for \"Non-\", \"Mid-\", \"Bi-\" if needed\n",
        "    formatted = re.sub(r\"(?<=\\w)(?=\\b(?:Non|Mid|Bi)-)\", \"-\", formatted)\n",
        "\n",
        "    # Remove space after \"Non \", \"Mid \", \"Bi \" and replace it with a hyphen\n",
        "    formatted = re.sub(r\"(Non|Mid|Bi)\\s\", r\"\\1-\", formatted)\n",
        "\n",
        "    # Special case: Handle \"Sign and Trade\" as a unique exception\n",
        "    formatted = re.sub(r\"Sign and Trade\", \"Sign-and-Trade\", formatted)\n",
        "\n",
        "    return formatted\n",
        "\n",
        "# Function to scrape player data from the player's individual page\n",
        "def scrape_player_data(player_link, player_key, player_name):\n",
        "    try:\n",
        "        # Send a GET request to the player's page\n",
        "        page = requests.get(player_link)\n",
        "        soup = BeautifulSoup(page.content, \"html.parser\")  # Parse the HTML content of the page\n",
        "\n",
        "        # CSS selector to find the \"Signed Using\" contract information\n",
        "        signed_using_selector = \"#contracts > div > div > div.contract-wrapper.mb-5 > div.contract-details.row.m-0 > div:nth-child(5) > div.label\"\n",
        "        # Find the corresponding HTML element using the selector\n",
        "        signed_using_element = soup.select_one(signed_using_selector)\n",
        "\n",
        "        # Get the text of the next sibling element containing the actual contract information\n",
        "        signed_using_value = signed_using_element.find_next_sibling().get_text().strip() if signed_using_element else None\n",
        "\n",
        "        # Format the extracted contract data using the format_signed function\n",
        "        cleaned_value = format_signed(signed_using_value)\n",
        "\n",
        "        # Return a dictionary containing the player data with the cleaned \"Signed Using\" value\n",
        "        return {\n",
        "            \"Player\": player_name,\n",
        "            \"Player Link\": player_link,\n",
        "            \"Player Key\": player_key,\n",
        "            \"Signed Using\": cleaned_value\n",
        "        }\n",
        "    except Exception as e:\n",
        "        # If an error occurs (e.g., page structure changes), return None for contract data\n",
        "        return {\n",
        "            \"Player\": player_name,\n",
        "            \"Player Link\": player_link,\n",
        "            \"Player Key\": player_key,\n",
        "            \"Signed Using\": None\n",
        "        }\n",
        "\n",
        "# Output file where the scraped data will be saved\n",
        "output_csv = \"signed_data.csv\"\n",
        "# Initialize the output CSV file with headers\n",
        "pd.DataFrame(columns=[\"Player\", \"Player Link\", \"Player Key\", \"Signed Using\"]).to_csv(output_csv, index=False, mode=\"w\", encoding=\"utf-8\", quoting=1)\n",
        "\n",
        "# Loop through each unique player link and scrape the data\n",
        "for idx, link in enumerate(unique_links):\n",
        "    # Extract player key and player name from the active data DataFrame\n",
        "    player_key = active_data[active_data[\"Player Link\"] == link][\"Player Key\"].values[0]\n",
        "    player_name = active_data[active_data[\"Player Link\"] == link][\"Player\"].values[0]\n",
        "\n",
        "    # Scrape the player's contract data using the scrape_player_data function\n",
        "    scraped_row = scrape_player_data(link, player_key, player_name)\n",
        "    # Append the scraped data to the output CSV file, replacing the \"Signed Using\" column with the cleaned data\n",
        "    pd.DataFrame([scraped_row]).to_csv(output_csv, mode=\"a\", header=False, index=False, encoding=\"utf-8\", quoting=1)\n",
        "\n",
        "    # Print progress as players are processed\n",
        "    print(f\"Processed {idx + 1}/{len(unique_links)} players ({((idx + 1) / len(unique_links)) * 100:.2f}%): {player_name}\")\n",
        "\n",
        "# Get the current datetime in the local timezone\n",
        "import pytz\n",
        "import datetime\n",
        "timezone = pytz.timezone(\"America/Chicago\")  # Replace with your local timezone\n",
        "current_time = datetime.datetime.now(timezone).strftime(\"%Y-%m-%d %H:%M:%S %Z%z\")\n",
        "\n",
        "# Print the completion message with timestamp and timezone\n",
        "print(f\"Data saved to {output_csv} at {current_time}\")\n"
      ],
      "metadata": {
        "id": "QVHmu5ytzelw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ac0062d-85db-443f-d735-98eb3bd18f6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1/439 players (0.23%): Aaron Gordon\n",
            "Processed 2/439 players (0.46%): Aaron Holiday\n",
            "Processed 3/439 players (0.68%): Aaron Nesmith\n",
            "Processed 4/439 players (0.91%): Aaron Wiggins\n",
            "Processed 5/439 players (1.14%): Adem Bona\n",
            "Processed 6/439 players (1.37%): AJ Green\n",
            "Processed 7/439 players (1.59%): AJ Johnson\n",
            "Processed 8/439 players (1.82%): Al Horford\n",
            "Processed 9/439 players (2.05%): Alec Burks\n",
            "Processed 10/439 players (2.28%): Alex Caruso\n",
            "Processed 11/439 players (2.51%): Alex Len\n",
            "Processed 12/439 players (2.73%): Alex Sarr\n",
            "Processed 13/439 players (2.96%): Alperen Sengun\n",
            "Processed 14/439 players (3.19%): Amen Thompson\n",
            "Processed 15/439 players (3.42%): Amir Coffey\n",
            "Processed 16/439 players (3.64%): Andre Drummond\n",
            "Processed 17/439 players (3.87%): Andre Jackson Jr.\n",
            "Processed 18/439 players (4.10%): Andrew Nembhard\n",
            "Processed 19/439 players (4.33%): Andrew Wiggins\n",
            "Processed 20/439 players (4.56%): Anfernee Simons\n",
            "Processed 21/439 players (4.78%): Anthony Black\n",
            "Processed 22/439 players (5.01%): Anthony Davis\n",
            "Processed 23/439 players (5.24%): Anthony Edwards\n",
            "Processed 24/439 players (5.47%): Anthony Gill\n",
            "Processed 25/439 players (5.69%): Antonio Reeves\n",
            "Processed 26/439 players (5.92%): Ariel Hukporti\n",
            "Processed 27/439 players (6.15%): Ausar Thompson\n",
            "Processed 28/439 players (6.38%): Austin Reaves\n",
            "Processed 29/439 players (6.61%): Ayo Dosunmu\n",
            "Processed 30/439 players (6.83%): Bam Adebayo\n",
            "Processed 31/439 players (7.06%): Baylor Scheierman\n",
            "Processed 32/439 players (7.29%): Ben Sheppard\n",
            "Processed 33/439 players (7.52%): Ben Simmons\n",
            "Processed 34/439 players (7.74%): Bennedict Mathurin\n",
            "Processed 35/439 players (7.97%): Bilal Coulibaly\n",
            "Processed 36/439 players (8.20%): Blake Wesley\n",
            "Processed 37/439 players (8.43%): Bobby Portis\n",
            "Processed 38/439 players (8.66%): Bobi Klintman\n",
            "Processed 39/439 players (8.88%): Bogdan Bogdanovic\n",
            "Processed 40/439 players (9.11%): Bojan Bogdanovic\n",
            "Processed 41/439 players (9.34%): Bol Bol\n",
            "Processed 42/439 players (9.57%): Bradley Beal\n",
            "Processed 43/439 players (9.79%): Brandin Podziemski\n",
            "Processed 44/439 players (10.02%): Brandon Clarke\n",
            "Processed 45/439 players (10.25%): Brandon Ingram\n",
            "Processed 46/439 players (10.48%): Brandon Miller\n",
            "Processed 47/439 players (10.71%): Brice Sensabaugh\n",
            "Processed 48/439 players (10.93%): Bronny James\n",
            "Processed 49/439 players (11.16%): Brook Lopez\n",
            "Processed 50/439 players (11.39%): Bruce Brown Jr.\n",
            "Processed 51/439 players (11.62%): Bruno Fernando\n",
            "Processed 52/439 players (11.85%): Bub Carrington\n",
            "Processed 53/439 players (12.07%): Buddy Hield\n",
            "Processed 54/439 players (12.30%): Cade Cunningham\n",
            "Processed 55/439 players (12.53%): Caleb Houstan\n",
            "Processed 56/439 players (12.76%): Caleb Martin\n",
            "Processed 57/439 players (12.98%): Cam Reddish\n",
            "Processed 58/439 players (13.21%): Cam Whitmore\n",
            "Processed 59/439 players (13.44%): Cameron Christie\n",
            "Processed 60/439 players (13.67%): Cameron Johnson\n",
            "Processed 61/439 players (13.90%): Cameron Payne\n",
            "Processed 62/439 players (14.12%): Cameron Thomas\n",
            "Processed 63/439 players (14.35%): Caris LeVert\n",
            "Processed 64/439 players (14.58%): Cason Wallace\n",
            "Processed 65/439 players (14.81%): Charles Bassey\n",
            "Processed 66/439 players (15.03%): Chet Holmgren\n",
            "Processed 67/439 players (15.26%): Chris Boucher\n",
            "Processed 68/439 players (15.49%): Chris Duarte\n",
            "Processed 69/439 players (15.72%): Chris Livingston\n",
            "Processed 70/439 players (15.95%): Chris Paul\n",
            "Processed 71/439 players (16.17%): Christian Braun\n",
            "Processed 72/439 players (16.40%): Christian Wood\n",
            "Processed 73/439 players (16.63%): C.J. McCollum\n",
            "Processed 74/439 players (16.86%): Clint Capela\n",
            "Processed 75/439 players (17.08%): Coby White\n",
            "Processed 76/439 players (17.31%): Cody Martin\n",
            "Processed 77/439 players (17.54%): Cody Williams\n",
            "Processed 78/439 players (17.77%): Cody Zeller\n",
            "Processed 79/439 players (18.00%): Colby Jones\n",
            "Processed 80/439 players (18.22%): Cole Anthony\n",
            "Processed 81/439 players (18.45%): Collin Sexton\n",
            "Processed 82/439 players (18.68%): Corey Kispert\n",
            "Processed 83/439 players (18.91%): Cory Joseph\n",
            "Processed 84/439 players (19.13%): Craig Porter Jr.\n",
            "Processed 85/439 players (19.36%): Dalano Banton\n",
            "Processed 86/439 players (19.59%): Dalen Terry\n",
            "Processed 87/439 players (19.82%): Dalton Knecht\n",
            "Processed 88/439 players (20.05%): Damian Lillard\n",
            "Processed 89/439 players (20.27%): Damion Lee\n",
            "Processed 90/439 players (20.50%): D'Angelo Russell\n",
            "Processed 91/439 players (20.73%): Daniel Gafford\n",
            "Processed 92/439 players (20.96%): Daniel Theis\n",
            "Processed 93/439 players (21.18%): Dante Exum\n",
            "Processed 94/439 players (21.41%): DaQuan Jeffries\n",
            "Processed 95/439 players (21.64%): Dario Saric\n",
            "Processed 96/439 players (21.87%): Dariq Whitehead\n",
            "Processed 97/439 players (22.10%): Darius Garland\n",
            "Processed 98/439 players (22.32%): DaRon Holmes II\n",
            "Processed 99/439 players (22.55%): David Roddy\n",
            "Processed 100/439 players (22.78%): Davion Mitchell\n",
            "Processed 101/439 players (23.01%): Day'Ron Sharpe\n",
            "Processed 102/439 players (23.23%): De'Aaron Fox\n",
            "Processed 103/439 players (23.46%): Dean Wade\n"
          ]
        }
      ]
    }
  ]
}