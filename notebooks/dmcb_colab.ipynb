{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPt3CJf+r7qd7NHnmHEppoO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b-harr/dmcb/blob/develop/notebooks/dmcb_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scripts"
      ],
      "metadata": {
        "id": "lN4nTFzdOsKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get_spotrac_contracts.py\n",
        "\n",
        "`get_spotrac_contracts.py` is a Python script that scrapes NBA player contract data from https://www.spotrac.com/nba/{team}/yearly for all 30 teams. It extracts relevant contract details then processes and saves this contract data to a CSV file (`data/spotrac_contracts.csv`) for use in salary cap management for the league."
      ],
      "metadata": {
        "id": "rqYovzQEFwUR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Zi4GPANzFgs-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import logging\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Get the root project directory (2 levels up from the current script)\n",
        "base_dir = os.getcwd()\n",
        "\n",
        "# Append the base_dir to sys.path to ensure modules can be imported\n",
        "sys.path.append(base_dir)\n",
        "\n",
        "# Import custom modules for the script\n",
        "import config\n",
        "from utils.text_formatter import make_player_key, format_text\n",
        "\n",
        "# Configure logging to capture detailed script execution and errors\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "logger = logging.getLogger()\n",
        "\n",
        "# Log the script start with a timestamp to track execution\n",
        "logger.info(\"The script started successfully.\")\n",
        "\n",
        "# Load environment variables from the .env file\n",
        "load_dotenv()\n",
        "logger.info(\"Environment variables loaded successfully.\")\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# List of NBA teams to scrape salary data for (from Spotrac)\n",
        "teams = [\n",
        "    \"atlanta-hawks\", \"brooklyn-nets\", \"boston-celtics\", \"charlotte-hornets\",\n",
        "    \"cleveland-cavaliers\", \"chicago-bulls\", \"dallas-mavericks\", \"denver-nuggets\",\n",
        "    \"detroit-pistons\", \"golden-state-warriors\", \"houston-rockets\", \"indiana-pacers\",\n",
        "    \"la-clippers\", \"los-angeles-lakers\", \"memphis-grizzlies\", \"miami-heat\",\n",
        "    \"milwaukee-bucks\", \"minnesota-timberwolves\", \"new-york-knicks\",\n",
        "    \"new-orleans-pelicans\", \"oklahoma-city-thunder\", \"orlando-magic\",\n",
        "    \"philadelphia-76ers\", \"phoenix-suns\", \"portland-trail-blazers\",\n",
        "    \"san-antonio-spurs\", \"sacramento-kings\", \"toronto-raptors\",\n",
        "    \"utah-jazz\", \"washington-wizards\"\n",
        "]\n",
        "\n",
        "# Define the directory and filename for saving the CSV file\n",
        "output_csv = config.spotrac_contracts_path\n",
        "\n",
        "# Set up a persistent session for making HTTP requests\n",
        "def get_session():\n",
        "    \"\"\"\n",
        "    Creates and returns a persistent session with appropriate headers\n",
        "    for making requests to Spotrac.\n",
        "    \"\"\"\n",
        "    session = requests.Session()\n",
        "    session.headers.update(\n",
        "        {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
        "    )\n",
        "    return session\n",
        "\n",
        "# Safe request function to handle HTTP errors and return the response\n",
        "def safe_request(session, url):\n",
        "    \"\"\"\n",
        "    Attempts to fetch data from the given URL, handling HTTP errors gracefully.\n",
        "    Returns the response if successful, otherwise logs an error.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = session.get(url)\n",
        "        response.raise_for_status()\n",
        "        return response\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        logging.warning(f\"HTTP error for {url}: {e}\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error fetching {url}: {e}\")\n",
        "    return None\n",
        "\n",
        "# Extract season headers (e.g., \"2024-25\", \"2023-24\") from the team's salary table\n",
        "def extract_season_headers(session, team):\n",
        "    \"\"\"\n",
        "    Extracts the season headers (e.g., \"2024-25\") from the salary table\n",
        "    for a given team on Spotrac. These headers represent the years for\n",
        "    the player's salary data.\n",
        "    \"\"\"\n",
        "    url = f\"https://www.spotrac.com/nba/{team}/yearly\"\n",
        "    response = safe_request(session, url)\n",
        "    if response:\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        table = soup.select_one(\"table\")\n",
        "        if table:\n",
        "            header_row = table.find(\"tr\")\n",
        "            if header_row:\n",
        "                # Extract headers that represent years\n",
        "                headers = [th.get_text(strip=True) for th in header_row.find_all(\"th\")]\n",
        "                season_headers = [header for header in headers if re.match(r\"^\\d{4}-\\d{2}$\", header)]\n",
        "                if season_headers:\n",
        "                    logging.info(f\"Season headers extracted for team: {team}\")\n",
        "                    return season_headers\n",
        "    logging.warning(f\"Failed to extract headers for team: {team}\")\n",
        "    return []\n",
        "\n",
        "# Extract player data (name, position, salary, etc.) from the salary table of the team\n",
        "def extract_player_data(session, team, season_headers):\n",
        "    \"\"\"\n",
        "    Extracts player data (name, position, salary, etc.) from the salary table\n",
        "    of the given team on Spotrac, limiting to the first 5 salary years.\n",
        "    \"\"\"\n",
        "    url = f\"https://www.spotrac.com/nba/{team}/yearly\"\n",
        "    team_name = format_text(team)\n",
        "    response = safe_request(session, url)\n",
        "    if not response:\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    table = soup.select_one(\"table\")\n",
        "    if not table:\n",
        "        logging.warning(f\"No table found for team {team}\")\n",
        "        return []\n",
        "\n",
        "    rows = table.find_all(\"tr\")\n",
        "    team_data = []\n",
        "\n",
        "    for row in rows[1:]:\n",
        "        cols = row.find_all(\"td\")\n",
        "        if len(cols) < 4:\n",
        "            continue\n",
        "\n",
        "        # Extract player name and other details\n",
        "        player_name_tag = cols[0].find(\"a\")\n",
        "        if not player_name_tag:\n",
        "            continue\n",
        "\n",
        "        player_name = player_name_tag.get_text(strip=True)\n",
        "        player_link = player_name_tag[\"href\"]\n",
        "        player_key = make_player_key(player_name)\n",
        "        position = cols[1].get_text(strip=True)\n",
        "        age = cols[2].get_text(strip=True)\n",
        "\n",
        "        salary_data = []\n",
        "        # Extract salary data (e.g., \"$10M\", \"UFA\", etc.)\n",
        "        for col in cols[3:]:\n",
        "            cell_text = col.get_text(strip=True)\n",
        "            if \"Two-Way\" in cell_text:\n",
        "                salary_data.append(\"Two-Way\")\n",
        "            elif \"UFA\" in cell_text:\n",
        "                salary_data.append(\"UFA\")\n",
        "            elif \"RFA\" in cell_text:\n",
        "                salary_data.append(\"RFA\")\n",
        "            else:\n",
        "                salary_matches = re.findall(r\"\\$\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?\", cell_text)\n",
        "                salary_data.extend([s.replace(\",\", \"\") for s in salary_matches])\n",
        "\n",
        "        # Only keep the first 5 salaries (and pad with empty strings if fewer)\n",
        "        player_data = [player_name, player_link, player_key, team_name, url, position, age] + salary_data[:5]\n",
        "        player_data += [\"\"] * (5 - len(salary_data))  # Fill missing salary slots with empty strings\n",
        "        team_data.append(player_data)\n",
        "\n",
        "        # Log each player being processed\n",
        "        logging.info(f\"Processed player: {player_name}, Position: {position}, Salary: {', '.join(salary_data[:5])}\")\n",
        "\n",
        "    return team_data\n",
        "\n",
        "# Main function to scrape data for all teams and save to CSV\n",
        "def scrape_and_save_data():\n",
        "    \"\"\"\n",
        "    Scrapes salary data for all teams, processes the data, and saves it to a CSV file.\n",
        "    The CSV includes player names, links, positions, ages, and the first 5 years of salary data.\n",
        "    \"\"\"\n",
        "    session = get_session()\n",
        "    # Try extracting the season headers (representing the salary years)\n",
        "    season_headers = []\n",
        "    for team in teams:\n",
        "        season_headers = extract_season_headers(session, team)\n",
        "        if season_headers:\n",
        "            break\n",
        "\n",
        "    if not season_headers:\n",
        "        raise ValueError(\"Failed to extract season headers.\")\n",
        "\n",
        "    # Headers include the player details and first 5 salary years\n",
        "    headers = [\"Player\", \"Player Link\", \"Player Key\", \"Team\", \"Team Link\", \"Position\", \"Age\"] + season_headers[:5]  # Limit to first 5 seasons\n",
        "    pd.DataFrame(columns=headers).to_csv(output_csv, index=False, mode=\"w\", encoding=\"utf-8\")\n",
        "    logging.info(f\"CSV header written to {output_csv}\")\n",
        "\n",
        "    # Collect all player data across teams\n",
        "    all_data = []\n",
        "    for idx, team in enumerate(teams):\n",
        "        progress = (idx + 1) / len(teams) * 100\n",
        "        logging.info(f\"Processing team {idx+1}/{len(teams)} ({progress:.2f}%) - {team}\")\n",
        "        team_data = extract_player_data(session, team, season_headers)\n",
        "        all_data.extend(team_data)\n",
        "\n",
        "    # Log after all teams are processed\n",
        "    logging.info(\"All teams processed. Sorting player data...\")\n",
        "\n",
        "    # Sort player data by player name (Player Key)\n",
        "    sorted_data = sorted(all_data, key=lambda x: x[2].lower())\n",
        "    # Write the sorted data to CSV\n",
        "    pd.DataFrame(sorted_data, columns=headers).to_csv(output_csv, index=False, mode=\"w\", encoding=\"utf-8\")\n",
        "    logging.info(f\"Data processing completed. Data successfully written to the file: {output_csv}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the scraping and data-saving process\n",
        "    scrape_and_save_data()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sync_bbref_stats.csv\n",
        "\n",
        "Description"
      ],
      "metadata": {
        "id": "YT6NgfgkPX7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import logging\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Get the root project directory (2 levels up from the current script)\n",
        "base_dir = os.getcwd()\n",
        "\n",
        "# Append the base_dir to sys.path to make sure modules can be imported\n",
        "sys.path.append(base_dir)\n",
        "\n",
        "# Import custom modules for the script\n",
        "import config\n",
        "from utils.google_sheets_manager import GoogleSheetsManager\n",
        "from utils.csv_handler import CSVHandler\n",
        "from utils.data_fetcher import fetch_data, parse_html\n",
        "from utils.text_formatter import make_player_key\n",
        "\n",
        "# Configure logging to capture detailed script execution and errors\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,  # Log messages with level INFO and above\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "logger = logging.getLogger()\n",
        "\n",
        "# Log the script start with a timestamp to track execution\n",
        "logger.info(\"The script started successfully.\")\n",
        "\n",
        "# Load environment variables from the .env file\n",
        "load_dotenv()\n",
        "logger.info(\"Environment variables loaded successfully.\")\n",
        "\n",
        "# Retrieve necessary configuration values from the config module\n",
        "google_sheets_url = config.google_sheets_url\n",
        "sheet_name = \"Stats\"  # Name of the sheet where data will be written\n",
        "numeric_columns = \"PTS,TRB,AST,STL,BLK,TOV,PF,G,MP\".split(\",\")  # Columns to be used for numerical calculations\n",
        "logger.debug(f\"Using numeric columns: {numeric_columns}\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def main(year=2025):\n",
        "    \"\"\"\n",
        "    Main function to fetch, process, and store NBA player stats for the given year.\n",
        "    It fetches data from Basketball-Reference, processes it, calculates fantasy points,\n",
        "    and stores the result both in a CSV file and Google Sheets (only for the default year).\n",
        "    \"\"\"\n",
        "    # Construct the URL for the requested year's player stats page on Basketball-Reference\n",
        "    url = f\"https://www.basketball-reference.com/leagues/NBA_{year}_totals.html\"\n",
        "    # Set the request headers for fetching data\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    logger.info(f\"Fetching data from URL: {url}\")\n",
        "\n",
        "    # Fetch the HTML data from the Basketball-Reference website\n",
        "    try:\n",
        "        response = fetch_data(url, headers)\n",
        "        logger.info(f\"Data fetched successfully from {url}.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching data: {e}\")\n",
        "        return\n",
        "\n",
        "    # Parse the HTML content to extract player stats into a DataFrame\n",
        "    try:\n",
        "        df = parse_html(response)\n",
        "        logger.info(\"HTML parsed successfully into a DataFrame.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error parsing HTML: {e}\")\n",
        "        return\n",
        "\n",
        "    # Exclude 'League Average' rows to focus on individual player stats\n",
        "    df = df[df[\"Player\"] != \"League Average\"]\n",
        "    logger.info(f\"Excluded 'League Average' rows. Data size is now {len(df)} rows.\")\n",
        "\n",
        "    # Check for and drop any rows with missing player names\n",
        "    missing_players = df[df[\"Player\"].isna()]\n",
        "    if not missing_players.empty:\n",
        "        logger.warning(f\"Dropped {len(missing_players)} rows with missing player data.\")\n",
        "    df = df.dropna(subset=[\"Player\"])\n",
        "\n",
        "    # Add a 'Player Key' column by applying the make_player_key function to the 'Player' column\n",
        "    df[\"Player Key\"] = df[\"Player\"].apply(make_player_key)\n",
        "    logger.info(\"Added 'Player Key' column to DataFrame.\")\n",
        "\n",
        "    # Sort the DataFrame by 'Player Key' and 'Team' columns for organized output\n",
        "    df = df.sort_values(by=[\"Player Key\", \"Team\"])\n",
        "    logger.info(\"Sorted DataFrame by 'Player Key' and 'Team'.\")\n",
        "\n",
        "    # Drop duplicates based on 'Player Key', keeping the first occurrence\n",
        "    df = df.drop_duplicates(subset='Player Key', keep='first')\n",
        "    logger.info(\"Removed individual teams when more than one.\")\n",
        "\n",
        "    # Convert specified numeric columns to proper numeric types for calculations\n",
        "    df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors=\"coerce\")\n",
        "    logger.info(f\"Converted numeric columns: {numeric_columns} to numeric types.\")\n",
        "\n",
        "    # Replace any NaN values with 0 across the DataFrame to prevent errors in calculations\n",
        "    df.fillna(0, inplace=True)\n",
        "    logger.info(\"Filled NaN values with 0.\")\n",
        "\n",
        "    # Perform vectorized calculations to compute fantasy points and related stats\n",
        "    try:\n",
        "        # Calculate Fantasy Points (FP) as a sum of positive stats and negative ones\n",
        "        df[\"FP\"] = (\n",
        "            df[\"PTS\"] + df[\"TRB\"] + df[\"AST\"] +\n",
        "            df[\"STL\"] + df[\"BLK\"] - df[\"TOV\"] - df[\"PF\"]\n",
        "        ).astype(int)  # Fantasy Points: PTS/REB/AST/STL/BLK +1, TO/PF/TF -1\n",
        "        df[\"FPPG\"] = (df[\"FP\"] / df[\"G\"]).round(1)  # Fantasy Points Per Game\n",
        "        df[\"FPPM\"] = (df[\"FP\"] / df[\"MP\"]).round(2)  # Fantasy Points Per Minute\n",
        "        df[\"MPG\"] = (df[\"MP\"] / df[\"G\"]).round(1)  # Minutes Per Game\n",
        "        df[\"FPR\"] = ((df[\"FP\"] ** 2) / (df[\"G\"] * df[\"MP\"])).round(1)  # Fantasy Point Rating = FPPG * FPPM = (FP ** 2) / (G * MP)\n",
        "\n",
        "        logger.info(\"Calculated new fantasy stats (FP, FPPG, FPPM, MPG, FPR).\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during calculations: {e}\")\n",
        "        return\n",
        "\n",
        "    # Define the file path where the processed data will be saved\n",
        "    if year == 2025:  # Only sync to Google Sheets for the default year\n",
        "        output_csv = config.bbref_stats_path\n",
        "        logger.info(f\"Saving data to CSV file: {output_csv}\")\n",
        "        # Save the processed data to a CSV file\n",
        "        try:\n",
        "            CSVHandler.write_csv(output_csv, df.values.tolist(), headers=df.columns.tolist())\n",
        "            logger.info(f\"Data successfully written to the CSV file: {output_csv}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving data to CSV: {e}\")\n",
        "            return\n",
        "\n",
        "        # Authenticate and update the Google Sheets with the processed data\n",
        "        try:\n",
        "            # Get the current timestamp to indicate when the data was last updated\n",
        "            timestamp = logging.Formatter('%(asctime)s').format(logging.LogRecord(\"\", 0, \"\", 0, \"\", [], None))  # Get the current timestamp\n",
        "\n",
        "            # Initialize Google Sheets manager and clear existing data in the sheet\n",
        "            sheets_manager = GoogleSheetsManager()\n",
        "            sheets_manager.clear_data(sheet_name=\"Stats\")\n",
        "            logger.info(f\"Cleared existing data in Google Sheets '{sheet_name}'.\")\n",
        "\n",
        "            # Write the timestamp to Google Sheets\n",
        "            sheets_manager.write_data([[f\"Last updated {timestamp} by {config.service_account_email}\"]], sheet_name=\"Stats\", start_cell=\"A1\")\n",
        "            logger.info(\"Wrote timestamp to Google Sheets.\")\n",
        "\n",
        "            # Write the processed data to the 'Stats' sheet\n",
        "            sheets_manager.write_data([df.columns.tolist()] + df.values.tolist(), sheet_name=\"Stats\", start_cell=\"A2\")\n",
        "            logger.info(f\"Data successfully written to the '{sheet_name}' sheet.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error updating Google Sheets: {e}\")\n",
        "\n",
        "    else:\n",
        "        # Generate a dynamic alternate file path using the year\n",
        "        alternate_output_path = f\"data/bbref_stats_{year}.csv\"  # Path for alternate CSV output\n",
        "        logger.info(f\"Saving data to alternate CSV file: {alternate_output_path}\")\n",
        "        try:\n",
        "            CSVHandler.write_csv(alternate_output_path, df.values.tolist(), headers=df.columns.tolist())\n",
        "            logger.info(f\"Data successfully written to alternate CSV file: {alternate_output_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving data to alternate CSV: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    #main(year=2024)\n",
        "    #main(year=2023)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EaHIQJJPesR",
        "outputId": "184886b4-7d94-4172-e893-71af6d5140d0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:utils.csv_handler:Wrote headers to /content/data/bbref_stats.csv: ['Player', 'Age', 'Team', 'Pos', 'G', 'GS', 'MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', '2P', '2PA', '2P%', 'eFG%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'Trp-Dbl', 'Awards', 'Player Key', 'FP', 'FPPG', 'FPPM', 'MPG', 'FPR']\n",
            "INFO:utils.csv_handler:Wrote 485 rows to /content/data/bbref_stats.csv.\n",
            "INFO:utils.google_sheets_manager:Initializing GoogleSheetsManager...\n",
            "INFO:utils.google_sheets_manager:Successfully connected to Google Sheets. Service Account Email: gchelp@dmcb-442123.iam.gserviceaccount.com\n",
            "INFO:utils.google_sheets_manager:Accessed worksheet: Stats\n",
            "INFO:utils.google_sheets_manager:Cleared data from worksheet 'Stats'.\n",
            "INFO:utils.google_sheets_manager:Accessed worksheet: Stats\n",
            "INFO:utils.google_sheets_manager:Written data to worksheet 'Stats' starting at 'A1'.\n",
            "INFO:utils.google_sheets_manager:Accessed worksheet: Stats\n",
            "INFO:utils.google_sheets_manager:Written data to worksheet 'Stats' starting at 'A2'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Colab"
      ],
      "metadata": {
        "id": "0tr0NYS2INxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Upload the `.env` file to Colab"
      ],
      "metadata": {
        "id": "89sO2K6PIfsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "JIw7An__ITrM",
        "outputId": "16c2cf8b-a178-4248-828f-a4c927ab3447"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b187fc0c-b6d4-44ef-ab6f-72f29b28f518\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b187fc0c-b6d4-44ef-ab6f-72f29b28f518\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dmcb-442123-966817b53d6f.json to dmcb-442123-966817b53d6f.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Remove file in Colab\n",
        "The `-f` flag ensures that no error is raised if the file doesn't exist"
      ],
      "metadata": {
        "id": "Og2dKciUJP-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f dmcb-442123-966817b53d6f.json"
      ],
      "metadata": {
        "id": "2F3uIplOJVnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Install `python-dotenv`"
      ],
      "metadata": {
        "id": "JMeK0V9BIc-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkMvtGOBIqsw",
        "outputId": "69b1347b-506f-454b-d2e0-1bb51da2557d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Load the `.env` file"
      ],
      "metadata": {
        "id": "mSBUYvR-I0CY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load the .env file (assuming the file is in the current directory)\n",
        "load_dotenv('.env')\n",
        "\n",
        "# Access a specific environment variable\n",
        "my_api_key = os.getenv('GOOGLE_SHEETS_CREDENTIALS')  # Replace with the actual variable name\n",
        "\n",
        "print(my_api_key)  # Test that it works\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spDLy5CeI2fa",
        "outputId": "7eaa531c-cb3a-4f25-c268-28f25448c72d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "secrets/dmcb-442123-966817b53d6f.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload your secrets directory to Google Drive"
      ],
      "metadata": {
        "id": "DziHkHvPKZ2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Mount Google Drive to Colab\n"
      ],
      "metadata": {
        "id": "tiXnJN0QLxh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "Zec3BfBpL5md",
        "outputId": "6b9c4ccf-8eac-427c-c273-dc6446590534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Access the uploaded folder in Colab\n",
        "\n"
      ],
      "metadata": {
        "id": "UU9DR5HMLaus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "cwd = os.getcwd()\n",
        "secrets_dir = '/content/secrets'\n",
        "\n",
        "print(f\"The current working directory is: {cwd}\")\n",
        "print(f\"The secrets directory is: {cwd}\")\n",
        "\n",
        "# Check if the secrets directory exists\n",
        "if os.path.exists(cwd):\n",
        "    print(f\"The directory {cwd} exists.\")\n",
        "    print(os.listdir(cwd))\n",
        "else:\n",
        "    print(f\"The directory {cwd} does not exist.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRv3VTfMKhqq",
        "outputId": "15485312-065e-4bca-a709-724f4cd7de3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The current working directory is: /content\n",
            "The secrets directory is: /content\n",
            "The directory /content exists.\n",
            "['.config', '.ipynb_checkpoints', 'utils', 'secrets', '.env', '__pycache__', 'scripts', 'config.py', 'sample_data']\n"
          ]
        }
      ]
    }
  ]
}